{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Recurrent Neural Networks\n",
    "### Starter Code\n",
    "* **PyData Bristol - 5th Meetup:** https://www.meetup.com/PyData-Bristol/events/255667468/\n",
    "* **Event URL:** https://www.eventbrite.co.uk/e/intro-to-recurrent-neural-networks-tickets-52401888459\n",
    "* **Date:** Tue 13th November 2018\n",
    "* **Instructor:** John Sandall\n",
    "* **Contact:** john@coefficient.ai / @john_sandall\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install seaborn numpy pandas matplotlib pathlib sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from sklearn import datasets, ensemble, linear_model, model_selection, neighbors, metrics, preprocessing, neural_network\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Build A Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a basic RNN using just numpy. We won't train it for now, we'll instead just get a feeling for how it's working. We'll use input data that has 20 samples, each with two-features, and two time points (t=0 and t=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_samples = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,   5],\n",
       "       [-10,  -7],\n",
       "       [ -7,  -3],\n",
       "       [ -1,   9],\n",
       "       [  8,  -6],\n",
       "       [ -4,   2],\n",
       "       [ -9,  -4],\n",
       "       [ -3,   4],\n",
       "       [  7,  -5],\n",
       "       [  3,  -2],\n",
       "       [ -1,   9],\n",
       "       [  6,   9],\n",
       "       [ -5,   5],\n",
       "       [  5, -10],\n",
       "       [  8,  -7],\n",
       "       [  7,   9],\n",
       "       [  9,   9],\n",
       "       [  4,  -3],\n",
       "       [-10,  -9],\n",
       "       [ -1, -10]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our input data. Here's X at t=0\n",
    "X0 = np.random.randint(low=-10, high=10, size=(n_samples, n_features))\n",
    "X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly here's X at t=1\n",
    "X1 = np.random.randint(low=-10, high=10, size=(n_samples, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create the weight matrices `Wx` (connecting X to neurons) and `Wy` (connecting output y at t-1 to neurons at time t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -2,  2],\n",
       "       [ 0,  0, -5]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neurons = 3\n",
    "\n",
    "# Connects 2-features to 3-neurons\n",
    "Wx = np.random.randint(low=-5, high=5, size=(n_features, n_neurons))\n",
    "Wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4,  0,  4],\n",
       "       [-2, -5,  0],\n",
       "       [-5, -4, -3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connects 3-neuron output at time t-1 to 3-neurons at time t (the recurrent weights)\n",
    "Wy = np.random.randint(low=-5, high=5, size=(n_neurons, n_neurons))\n",
    "Wy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll also need the bias\n",
    "b = np.ones(n_neurons)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Calculate Y0!\n",
    "> \n",
    "> **Tips**:\n",
    "> - Remember `Y0 = activation(X0*Wx + b)` and `Y1 = activation(X0*Wx + Y0*Wy + b)`\n",
    "> - You'll need `np.matmul()` to do multiply two matrixes.\n",
    "> - You'll need `np.heaviside(some_vector, 0)` for your activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(m):\n",
    "    return m*np.heaviside(m, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0., -0., -0.],\n",
       "       [11., 21., 16.],\n",
       "       [ 8., 15.,  2.],\n",
       "       [ 2.,  3., -0.],\n",
       "       [-0., -0., 47.],\n",
       "       [ 5.,  9., -0.],\n",
       "       [10., 19.,  3.],\n",
       "       [ 4.,  7., -0.],\n",
       "       [-0., -0., 40.],\n",
       "       [-0., -0., 17.],\n",
       "       [ 2.,  3., -0.],\n",
       "       [-0., -0., -0.],\n",
       "       [ 6., 11., -0.],\n",
       "       [-0., -0., 61.],\n",
       "       [-0., -0., 52.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., 24.],\n",
       "       [11., 21., 26.],\n",
       "       [ 2.,  3., 49.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y0 = activation(np.matmul(X0,Wx) + b)\n",
    "Y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0., -0., -0.],\n",
       "       [-0., -0., 12.],\n",
       "       [-0., -0., 28.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0.,  3.],\n",
       "       [-0., -0., 34.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., -0.],\n",
       "       [-0., -0., -0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1 = activation(np.matmul(X0,Wx) + np.matmul(Y0,Wy) + b)\n",
    "Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Y1,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Build A Recurrent Neural Network using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work through a simple example now using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install keras tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from keras.layers import SimpleRNN, Dense, TimeDistributed\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9936168441113400842\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Check if Keras is using GPU version of TensorFlow\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at 5 time steps, with:\n",
    "- input X has 20 samples and two features\n",
    "- output y is of length 3 (we have three neurons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input format shape for Keras is (sample size, number of time steps, features)\n",
    "n_steps = 5\n",
    "\n",
    "X = np.random.randint(low=-10, high=10, size=(n_samples, n_steps, n_features))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.random.randint(low=-10, high=10, size=(n_samples, n_steps, n_neurons))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Define a simple `Sequential` RNN model using Keras\n",
    "> - The model should contain one layer (`SimpleRNN` with 3 units, and `return_sequences=True`\n",
    "> - Assign it to a variable called `model`\n",
    "> - Use the Keras documentation if you get stuck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model here...\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, return_sequences=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Compile & fit the model\n",
    "> - Use MSE loss and `rmsprop` optimizer.\n",
    "> - Fit it to X and y, using 10 epochs and batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 35.1137\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 158us/step - loss: 35.1033\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 165us/step - loss: 35.0961\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 160us/step - loss: 35.0904\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 144us/step - loss: 35.0855\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 151us/step - loss: 35.0811\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 160us/step - loss: 35.0771\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 210us/step - loss: 35.0734\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 193us/step - loss: 35.0700\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 202us/step - loss: 35.0667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11ef20090>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='MSE')\n",
    "model.fit(X, y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out! We'll generate some new data `X_new` in the same shape as X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll have one sample, so we want it to have shape (1, 5, 2)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This has shape (1, 5, 2)\n",
    "X_new = np.array([\n",
    "    [[1, 0],  # t = 0 (two features)\n",
    "     [0, 1],  # t = 1\n",
    "     [0, 1],  # t = 2\n",
    "     [0, 1],  # t = 3\n",
    "     [0, 1],  # t = 4\n",
    "    ]\n",
    "])\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.65920615,  0.23279989,  0.794165  ],\n",
       "        [-0.06249426,  0.76769173,  0.5828201 ],\n",
       "        [-0.7510873 ,  0.6860359 ,  0.5734208 ],\n",
       "        [-0.8786577 ,  0.3345276 ,  0.38988632],\n",
       "        [-0.8524856 ,  0.24765082,  0.03467968]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our RNN is able to predict some outcomes y of length 3, for each time step.\n",
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Predict single value outputs for y (instead of vectors of length 3)\n",
    "> - Within your `Sequential` model, add a fully connected `Dense()` network with `input_dim=1` and `output_dim=1`\n",
    "> - Compile as before\n",
    "> - Fit to the new y provided\n",
    "> - Predict for `X_new` again, confirming that your outputs are a single time series of 5 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want a newly shaped y to predict, containing 20 samples over 5 time steps, but otherwise scalar output.\n",
    "y = np.random.randint(low=-10, high=10, size=(n_samples, n_steps, 1))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 35.2705\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 146us/step - loss: 35.2290\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 171us/step - loss: 35.1982\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 185us/step - loss: 35.1720\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 178us/step - loss: 35.1485\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 255us/step - loss: 35.1266\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 200us/step - loss: 35.1061\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 197us/step - loss: 35.0864\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 204us/step - loss: 35.0674\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 227us/step - loss: 35.0491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120b7b910>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define your model here...\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, return_sequences=True))\n",
    "model.add(Dense(input_dim=1, output_dim=1))\n",
    "model.compile(optimizer='rmsprop', loss='MSE')\n",
    "model.fit(X, y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.5718623 ],\n",
       "        [ 0.9151196 ],\n",
       "        [-0.45066446],\n",
       "        [-0.12483628],\n",
       "        [-0.3880046 ]]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Train a more fully fledged RNN on real data.\n",
    "> - We'll construct an X input with `1` at t=0 and `0` otherwise.\n",
    "> - Our `y` output just has a simple pattern.\n",
    "> - The RNN should be able to learn the relationship between the X pattern, and the corresponding y pattern.\n",
    "> - Re-use your code from before, i.e. a Sequential model containing a SimpleRNN (this time with 50 units), plus a Dense layer with 1 unit and `sigmoid` activation.\n",
    "> - Compile as before, and fit to `x_train` and `y_train` using 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are our sequences. The RNN should learn to predict the\n",
    "# 0.8 and 0.6 correctly because it can remember the 1 in the inputs.\n",
    "x_seed = [1, 0, 0, 0, 0, 0]\n",
    "y_seed = [1, 0.8, 0.6, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6, 1)\n",
      "(1000, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "# Let's create 1000 identical samples.\n",
    "n_samples = 1000\n",
    "\n",
    "x_train = np.array([[x_seed] * n_samples]).reshape(n_samples, len(x_seed), 1)\n",
    "print(x_train.shape)\n",
    "y_train = np.array([[y_seed] * n_samples]).reshape(n_samples, len(y_seed), 1)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model here...\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(50, return_sequences=True))\n",
    "model.add(Dense(input_dim=1, output_dim=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile...\n",
    "model.compile(optimizer='rmsprop', loss='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s 877us/step - loss: 0.0732\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 102us/step - loss: 0.0232\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 101us/step - loss: 0.0087\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 101us/step - loss: 0.0033\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 135us/step - loss: 0.0015\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 8.3093e-04\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 101us/step - loss: 5.7460e-04\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 101us/step - loss: 4.5104e-04\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 102us/step - loss: 3.8233e-04\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 112us/step - loss: 3.3877e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12241d550>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit...\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's predict for this x_new\n",
    "x_new = np.array([[[1],[0],[0],[0],[0],[0]]])\n",
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.97347236],\n",
       "        [0.80541945],\n",
       "        [0.63428426],\n",
       "        [0.00627161],\n",
       "        [0.00542594],\n",
       "        [0.00383898]]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: LSTMs and GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Try using the LSTM and GRU units from Keras on the previous example. Does it appear to perform any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1426\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 269us/step - loss: 0.0789\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 263us/step - loss: 0.0499\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.0251\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 270us/step - loss: 0.0132\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 274us/step - loss: 0.0080\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 271us/step - loss: 0.0050\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 267us/step - loss: 0.0030\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 290us/step - loss: 0.0017\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 330us/step - loss: 9.1086e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122959650>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define your model here...\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(Dense(input_dim=1, output_dim=1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='MSE')\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1447\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 221us/step - loss: 0.0735\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 220us/step - loss: 0.0354\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 237us/step - loss: 0.0129\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 254us/step - loss: 0.0064\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 291us/step - loss: 0.0038\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 290us/step - loss: 0.0024\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 242us/step - loss: 0.0017\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 232us/step - loss: 0.0010\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 226us/step - loss: 5.9911e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124607150>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define your model here...\n",
    "model = Sequential()\n",
    "model.add(GRU(50, return_sequences=True))\n",
    "model.add(Dense(input_dim=1, output_dim=1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='MSE')\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Try adding some additional components from the example provided [on the Keras docs here](https://keras.io/getting-started/sequential-model-guide/), such as Dropout. How does this improve things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1415\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 270us/step - loss: 0.0796\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 276us/step - loss: 0.0505\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 260us/step - loss: 0.0273\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 242us/step - loss: 0.0157\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 244us/step - loss: 0.0103\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 264us/step - loss: 0.0069\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 279us/step - loss: 0.0049\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 337us/step - loss: 0.0035\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 320us/step - loss: 0.0024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12518dad0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define your model here...\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(input_dim=1, output_dim=1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='MSE')\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Suggested \"homework\" exercise: Work through the Keras \"text generation example\" code: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "> \n",
    "> Try applying this to your own text dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "606208/600901 [==============================] - 0s 1us/step\n",
      "614400/600901 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "path = get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('corpus length:', 600893)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total chars:', 57)\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb sequences:', 200285)\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      " 65408/200285 [========>.....................] - ETA: 1:43 - loss: 2.3761"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
